{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "data=np.float64(np.load('MNIST.npy'))\n",
    "labels=np.float32(np.load('MNIST_labels.npy'))\n",
    "n=len(data)\n",
    "p=len(data[0])\n",
    "for i in range(0,n):\n",
    "    data[i]=data[i]/255 # The value lie in [0,1]\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mnist data and split into train validation and test\n",
    "def get_mnist():\n",
    "\n",
    "    data=np.float64(np.load('MNIST.npy'))\n",
    "    labels=np.float32(np.load('MNIST_labels.npy'))\n",
    "    #print(data.shape)\n",
    "    \n",
    "    data=np.float32(data)/255.\n",
    "    \n",
    "    train_dat=data[0:50000]\n",
    "    train_labels=labels[0:50000]\n",
    "    \n",
    "    val_dat=data[50000:60000]\n",
    "    val_labels=labels[50000:60000]\n",
    "    \n",
    "    test_dat=data[60000:70000]\n",
    "    test_labels=labels[60000:70000]\n",
    "    \n",
    "    return (train_dat, train_labels), (val_dat, val_labels), (test_dat, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Train,TrainLabel),(Val,ValLabel),(Test,TestLabel)=get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time is about 23.6632\n"
     ]
    }
   ],
   "source": [
    "nTrees=MinimumErrorTree\n",
    "clfb=RandomForestClassifier(n_estimators=1,min_samples_split=2,max_features=100,criterion=\"entropy\")\n",
    "AdaB=AdaBoostClassifier(clfb,n_estimators=nTrees,algorithm=\"SAMME\")\n",
    "start_time = time.time()\n",
    "AdaB.fit(Train,TrainLabel)\n",
    "end_time = time.time()\n",
    "t=end_time-start_time\n",
    "print(\"The training time is about\",round(t,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training   error rate is  0.0001\n",
      "The validation error rate is  0.0511\n",
      "The test       error rate is  0.0477\n"
     ]
    }
   ],
   "source": [
    "temp=AdaB.predict(Train)\n",
    "e1=sum((temp!=TrainLabel))/len(temp)\n",
    "\n",
    "temp=AdaB.predict(Val)\n",
    "e2=sum((temp!=ValLabel))/len(temp)\n",
    "\n",
    "temp=AdaB.predict(Test)\n",
    "e3=sum((temp!=TestLabel))/len(temp)\n",
    "\n",
    "print(\"The training   error rate is \",round(e1,4))\n",
    "print(\"The validation error rate is \",round(e2,4))\n",
    "print(\"The test       error rate is \",round(e3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error rate is similarly with the result of Random Forest Classifier but test error rate is slightly larger than those of Random Forest Classifier.\n",
    "\n",
    "Since adaboost algorithm updates the weight by using misclassified training sample, it is easily overfitted. This is the reason why the error of training sample and the error of val, test have pretty large difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different boosting reweighting rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrees=MinimumErrorTree\n",
    "Weights=1/np.repeat(1,len(Train)) #Initial Weights\n",
    "Predict_Prob_Train=[]\n",
    "Predict_Prob_Val=[]\n",
    "TreeList=[]                      #Store each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training time is about 27.476\n"
     ]
    }
   ],
   "source": [
    "N=range(0,nTrees)\n",
    "start_time = time.time()\n",
    "for i in N:\n",
    "    if(i==0):\n",
    "        #Grow one tree\n",
    "        clf=RandomForestClassifier(n_estimators=1,min_samples_split=2,max_features=100,criterion=\"entropy\")\n",
    "        TreeList.append(clf)\n",
    "        TreeList[i].fit(Train,TrainLabel)\n",
    "        \n",
    "        #Update weights by using error rate for each sample\n",
    "        temp=TreeList[i].predict(Train)\n",
    "        e=sum((temp!=TrainLabel))/len(temp)\n",
    "        for j in range(len(temp)):\n",
    "            if(temp[j]!=TrainLabel[j]):\n",
    "                Weights[j]=Weights[j]/e\n",
    "        \n",
    "        #Probability of output by each trees\n",
    "        temp1=np.array(TreeList[i].predict_proba(Train))\n",
    "        temp2=np.array(TreeList[i].predict_proba(Val))\n",
    "        Predict_Prob_Train.append(temp1)\n",
    "        Predict_Prob_Val.append(temp2)\n",
    "    else:\n",
    "        #Grow one tree\n",
    "        clf=RandomForestClassifier(n_estimators=1,min_samples_split=2,max_features=100,criterion=\"entropy\")\n",
    "        TreeList.append(clf)\n",
    "        TreeList[i].fit(Train,TrainLabel,Weights)\n",
    "        \n",
    "        #Update weights by using error rate for each sample\n",
    "        temp=TreeList[i].predict(Train)\n",
    "        e=sum((temp!=TrainLabel))/len(temp)\n",
    "        for j in range(len(temp)):\n",
    "            if(temp[j]!=TrainLabel[j]):\n",
    "                Weights[j]=Weights[j]/e\n",
    "        \n",
    "        #Add terminal probability of all trees\n",
    "        temp1=np.array(TreeList[i].predict_proba(Train))\n",
    "        temp2=np.array(TreeList[i].predict_proba(Val))\n",
    "        Predict_Prob_Train=Predict_Prob_Train+temp1\n",
    "        Predict_Prob_Val=Predict_Prob_Val+temp2\n",
    "\n",
    "end_time = time.time()\n",
    "t=end_time-start_time\n",
    "print(\"The training time is about\",round(t,4))\n",
    "\n",
    "#Average the terminal probabilities of trees\n",
    "Predict_Prob_Train=np.array(Predict_Prob_Train)/nTrees\n",
    "Predict_Prob_Val=np.array(Predict_Prob_Val)/nTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1=0\n",
    "for i in range(len(Train)):\n",
    "    temp=Predict_Prob_Train[0][i].argmax() #Find the label with the highest probability\n",
    "    if(temp!=TrainLabel[i]):\n",
    "        e1+=1\n",
    "e1=e1/len(Train)\n",
    "\n",
    "e2=0\n",
    "for i in range(len(Val)):\n",
    "    temp=Predict_Prob_Val[0][i].argmax()\n",
    "    if(temp!=ValLabel[i]):\n",
    "        e2+=1\n",
    "e2=e2/len(Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict test data with all trees we saved\n",
    "\n",
    "def Predict_data(data):\n",
    "    Predict_Prob=np.zeros([len(data),len(set(TrainLabel))])\n",
    "    for i in TreeList:\n",
    "        Predict_Prob=Predict_Prob+np.array(i.predict_proba(data))\n",
    "    Predict_Prob=Predict_Prob/len(data)\n",
    "    \n",
    "    temp=[]\n",
    "    for i in range(len(data)):\n",
    "        temp.append(Predict_Prob[i].argmax())\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_Test=Predict_data(Test)\n",
    "e3=sum(Predict_Test!=TestLabel)/len(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training   error rate is  0.0\n",
      "The validation error rate is  0.0473\n",
      "The test       error rate is  0.041\n"
     ]
    }
   ],
   "source": [
    "print(\"The training   error rate is \",round(e1,4))\n",
    "print(\"The validation error rate is \",round(e2,4))\n",
    "print(\"The test       error rate is \",round(e3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change the weight updating method from AdaBoost. It also has very small training error rate. The validation and error rates are between those of Random Forest and those of AdaBoost. The new weight updating method shows the lower overfitted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
